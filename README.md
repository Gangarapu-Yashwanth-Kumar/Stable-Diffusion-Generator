# ğŸ¨ğŸ§  Stable Diffusion Generator

A multimodal generative AI application that transforms text prompts into **high-quality images** ğŸ–¼ï¸ and **audio** ğŸ”Š using **Stable Diffusion** models from ğŸ¤— Hugging Face. Built with `diffusers`, `torch`, and `Gradio`, this project enables interactive and accessible content generation.

---

## ğŸš€ Features

- ğŸ” **Text-to-Image Generation** with `StableDiffusionXLPipeline`
- ğŸ”Š **Text-to-Audio Generation** using `StableAudioPipeline`
- âš¡ GPU-accelerated with PyTorch + CUDA
- ğŸŒ Gradio-based web UI for seamless user interaction
- ğŸ¯ Example prompts for testing both audio and visual outputs

---

## ğŸ‘¨â€ğŸ’» Responsibilities

As part of this project, I was responsible for:

- Integrating Hugging Face models for both text-to-image and text-to-audio tasks
- Implementing two interactive Gradio interfaces
- Optimizing the inference pipelines for GPU performance
- Structuring notebooks for clarity and experimentation
- Designing example use cases and prompts
- Writing clean, reproducible code for public sharing

---

## ğŸ“ˆ Outcomes

- âœ… Successfully generated high-quality audio and image outputs using text prompts
- ğŸ§ª Created a functional prototype for multimodal generation
- ğŸ–¼ï¸ Designed an intuitive UI for testing and demonstrating results
- ğŸ§° Packaged project into a reusable and extensible notebook-based toolkit

---

## ğŸ“š Learning Insights

- Gained hands-on experience with Hugging Face `diffusers` and model pipelines
- Explored different schedulers like `EulerDiscreteScheduler` to optimize outputs
- Learned efficient audio generation using `StableAudioPipeline` and `soundfile`
- Developed UI deployment experience through `gradio`
- Understood how prompts influence outputs in both visual and audio domains

---

## ğŸ§  Summary Comparison

| Feature                  | Text-to-Image                            | Text-to-Audio                            |
|--------------------------|------------------------------------------|------------------------------------------|
| ğŸ¤– Model                 | `StableDiffusionXLPipeline`              | `StableAudioPipeline`                    |
| ğŸ§© Model Source          | `stabilityai/stable-diffusion-xl-base`   | `stabilityai/stable-audio-open-1.0`      |
| ğŸ¯ Input                 | Prompt text                              | Prompt + negative prompt + duration      |
| ğŸ“¤ Output                | Image (`.png`, PIL)                      | Audio (`.wav`, NumPy array)              |
| ğŸ’» Interface             | Gradio UI                                | Gradio UI                                |
| ğŸš€ Runtime               | CUDA-accelerated via PyTorch             | CUDA-accelerated via PyTorch             |

---

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/your-username/stable-diffusion-generator.git
cd stable-diffusion-generator
```


## ğŸ” Hugging Face Authentication

To use pre-trained models from Hugging Face:
1. [Create an account](https://huggingface.co/join) if you don't have one.
2. Run the following inside the notebook or terminal:
```python
from huggingface_hub import notebook_login
notebook_login()
```

---

## ğŸ§ª Example Prompts

**Text-to-Image:**
> "A director filming a dramatic scene in the middle of a monsoon."

**Text-to-Audio:**
> "A baby crying loudly with heavy background noise."

---

## ğŸŒ Launch Gradio Interface

To run the web UI:
- For **image generation**, launch from `Stable Diffusion(Text-Image).ipynb`
- For **audio generation**, launch from `Stable Diffusion (Text-Audio).ipynb`

---


## ğŸ™ Thank You

Thank you for checking out **Stable Diffusion Generator**!  
Your interest in creative AI applications is what drives open innovation. Feel free to â­ star the repo, contribute, or share your outputs!

---
